{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning (2022), exercises\n",
    "\n",
    "\n",
    "## General instructions for all exercises\n",
    "\n",
    "Follow the instructions and fill in your solution under the line marked by tag\n",
    "\n",
    "> YOUR CODE HERE\n",
    "\n",
    "Remove also line \n",
    "\n",
    "> raise NotImplementedError()\n",
    "\n",
    "**Do not change other areas of the document**, since it may disturb the autograding of your results!\n",
    "  \n",
    "Having written the answer, execute the code cell by and pressing `Shift-Enter` key combination. The code is run, and it may print some information under the code cell. The focus automatically moves to the next cell and you may \"execute\" that cell by pressing `Shift-Enter` again, until you have reached the code cell which tests your solution. Execute that and follow the feedback. Usually it either says that the solution seems acceptable, or reports some errors. You can go back to your solution, modify it and repeat everything until you are satisfied. Then proceed to the next task.\n",
    "   \n",
    "Repeat the process for all tasks.\n",
    "\n",
    "The notebook may also contain manually graded answers. Write your manually graded answer under the line marked by tag:\n",
    "\n",
    "> YOUR ANSWER HERE\n",
    "\n",
    "Manually graded tasks are text in markdown format. It may contain text, pseudocode, or mathematical formulas. You can write formulas with $\\LaTeX$-syntax by enclosing the formula with dollar signs (`$`), for example `$f(x)=2 \\pi / \\alpha$`, will produce $f(x)=2 \\pi / \\alpha$\n",
    "\n",
    "When you have passed the tests in the notebook, and you are ready to submit your solutions, download the whole notebook, using menu `File -> Download as -> Notebook (.ipynb)`. Save the file in your hard disk, and submit it in [Moodle](https://moodle.uwasa.fi) or EUNICE Moodle under the corresponding excercise.\n",
    "\n",
    "Your solution should be an executable Python code. Use the code already existing as an example of Python programing and read more from the numerous Python programming material from the Internet if necessary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satellite image classification\n",
    "\n",
    "Following satellite image is obtained from Vaasa in 2.6.2021. The image is acquired from European Sentinell 2 satellite by means of multispectral imaging device (MSI). The multispectral camera has acquired the image using 13 different wavelength bands instead of three (RGB) in the normal camera. These images can searched and dowloaded using [Copernicus Open Access Hub](https://scihub.copernicus.eu/dhus/), and preprosessed by using ESA's [SNAP](http://step.esa.int/main/download/) tool. This data is However downloaded by Cem, using his extraordinary [satellite data tool](https://cemmozzy.users.earthengine.app/view/test).\n",
    "\n",
    "\n",
    "The bands used are \n",
    "\n",
    "| Band number | Band name | Wavelength | Region | Remarks |\n",
    "| ----------- | --------- | -----------| ------ | ------- |\n",
    "|  1 | B1  |   443 nm | Violet     | Chlorophyll-A |\n",
    "|  2 | B2  |   490 nm | Cyan       | |\n",
    "|  3 | B3  |   560 nm | Green      | |\n",
    "|  4 | B4  |   665 nm | Red        | Chlorophyll_A |\n",
    "|  5 | B5  |   705 nm | Red        | |\n",
    "|  6 | B6  |   740 nm | Red        | |\n",
    "|  7 | B7  |   783 nm | Deep red   | |\n",
    "|  8 | B8  |   842 nm | NIR        | |\n",
    "|  9 | B8A |   865 nm | NIR        | |\n",
    "| 10 | B9  |   945 nm | NIR        | |\n",
    "| 11 | B10 |  1375 nm | NIR        | |\n",
    "| 12 | B11 |  1610 nm | NIR        | |\n",
    "| 13 | B12 |  2190 nm | NIR        | |\n",
    "| 14 | -   |  -       | –          |Empty      | \n",
    "| 15 | -   |  -       | -          |Empty      |\n",
    "| 16 | -   |  -       | -          | Empty     |\n",
    "\n",
    "The channels listed above can be used for creating a natural looking RGB-image, as shown below.\n",
    "\n",
    "![Palosaari](Palosaari.png)\n",
    "\n",
    "Even though, only three channels are used for RGB image, all 13 can be usefull features for land type and crops classification. \n",
    "\n",
    "## Training data \n",
    "\n",
    "The training data is obtained from [Dynamic word land usage dataset](https://www.dynamicworld.app). \n",
    "\n",
    "The labelled areas are:\n",
    "\n",
    "| Segment no. | Segment name | Segment color | \n",
    "| ----------- | --------- | -----------| \n",
    "|  0 | Water   | Blue |\n",
    "|  1 | Trees   | Green |\n",
    "|  2 | Grass   | Light green |\n",
    "|  3 | Crops   | Brownish Yellow |\n",
    "|  4 | Shrub | Yello |\n",
    "|  5 | Flooded vegetation | Lila |\n",
    "|  6 | Built up area | Red |\n",
    "|  7 | Bare ground | Gray |\n",
    "|  8 | Snow & Ice | Lila |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "\n",
    "### Read the data\n",
    "Open the data which is in two 32-bit TIFF images. These images need to be opened using imageIo library with LZW compression support, neither installed by default and needs to be installed first.\n",
    "\n",
    "The data is stored in the following images:\n",
    " - `20210602_s2.tif`: The Sentinel two spectral image, 800x4817 pixels, each pixel containin 16 32-bit channels.\n",
    " - `20210602_dw.tif`: The Dynamic World land use classification data, 800x4817 pixels, each pixel containin one 8-bit integer. Values from 0:15\n",
    "\n",
    "\n",
    "Install first the needed libraries `imageio` and `imagecodecs` with pip.\n",
    "\n",
    "The opening of the images can be achieved like this:\n",
    "\n",
    "\n",
    "`import imageio as iio\n",
    "Is2 = iio.imread('20210602_s2.tif')\n",
    "Idw = iio.imread('20210602_dw.tif')\n",
    "`\n",
    "\n",
    "Select the Palosaari area by taking a subset from image using only pixels from $y\\in[200:800]$ and $x\\in[500:1000]$ and store it as `I`.\n",
    "\n",
    "Select the labels from the Dynamic world image Idw, covering the same area, and store it as a row-vector `l`. \n",
    "\n",
    "Plot the image using only channels 1, 2 and 3 to see which area it covers, and plot a histogram of area labels from Dynamic world to see how much there are samples from different areas in the image. Scale the image so that its values are floating point values between 0..1 to display it properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imageio\n",
    "#!pip install imagecodecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:19:10.665770Z",
     "start_time": "2021-11-08T09:19:06.791514Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the imageio-library which is also capable of reading 32 bit scientific TIFF images\n",
    "import imageio as iio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:19:16.196777Z",
     "start_time": "2021-11-08T09:19:16.160262Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "801e6afd86e668d20c718757f34bc28c",
     "grade": false,
     "grade_id": "cell-45f96d93d5d4ac3d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b45ddebd7926b08d09d002e1faff2812",
     "grade": true,
     "grade_id": "cell-056b58bdb592b733",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Tests\n",
    "\n",
    "points=0\n",
    "if ('Is2' in globals()) and ('l' in globals()):\n",
    "    points+=1\n",
    "else:\n",
    "    print(\"Please define Is2 and l\")\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Construct the design matrix `X`, label vector `y` and split the data to training and testing sets.\n",
    "\n",
    "Reshape the image data `I` so that is has only one spatial dimension, and first 13 features. Use the `reshape` function of numpy arrays for this purpose. Store your results to design matrix `X`.\n",
    "\n",
    "Check that your label vector `l` is already a row-vector. If it is not you can use `reshape` or `ravel` -functions of numpy array to convert it to row vector.\n",
    "\n",
    "Use sklearn function `train_test_split` to randomly split the data into testing set and training set. Normally it is good to use quite large training set, but since we are now using nearest neighbours method, with is slow with large training sets, we will exceptionally split the data so that 1% will be used as a training set and 99% for testing. The function splits both the design matrix `X` and the label vector `y` at the same time, to make it's use very convenient. Store your training sets to variables `X_train`, `y_train` and your testing set to variables `X_test` and `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1ed92c7b904f22721d5d77e2427cb86",
     "grade": false,
     "grade_id": "cell-e6b8b26aa9317bf1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:19:20.438581Z",
     "start_time": "2021-11-08T09:19:20.436195Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f387b768c9e5d333be5d6a26ee378c0",
     "grade": true,
     "grade_id": "cell-053c17a94ef0626d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Tests\n",
    "\n",
    "points=0\n",
    "if ('X_train' in globals()) and ('X_test' in globals()) and ('y_train' in globals()) and ('y_test' in globals()):\n",
    "    points+=0.5\n",
    "if (X_train.shape == (3000,13)) and (len(y_train)==3000):\n",
    "    points+=0.5\n",
    "\n",
    "assert(y.dtype==np.uint8)\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Analyze the complexity of the data  by plotting the training set of data with two first principal components\n",
    "\n",
    " - Transform the the trainig set data in PCA domain as `pc`\n",
    " - Plot a scatter plot of two first principal components, where the color is the label number, the integer representation of `y` (=classes) of the training set\n",
    " - Use the `s` (size) parameter to set the size of the dots in scatter plot smaller, to see the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:47:59.196422Z",
     "start_time": "2021-11-08T09:47:58.663270Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "435de7a3d0d7c078a16b9de4fba8026a",
     "grade": false,
     "grade_id": "cell-27739bd3a80987cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:55:01.599891Z",
     "start_time": "2021-11-08T09:55:01.593089Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fea264a3aebf1a45c9f53a4690c7203",
     "grade": true,
     "grade_id": "cell-e1276f5f5fb251e5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Tests\n",
    "\n",
    "assert(pc.shape==(3000, 2)), \"Something is wrong with PCA\"\n",
    "assert(max(y_train)<9), \"Something is wrong with integer labels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Define a KNN classifier which assigns each pixel from the image to the correct land use similar way than Dynamic World land use map.\n",
    "\n",
    "- Create a processing pipeline using standard scaler and KNN classifier, name the pipeline as `predictor`\n",
    "- Train the pipeline using the training data\n",
    "- Check if it passes the tests for precision in training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:55:05.352972Z",
     "start_time": "2021-11-08T09:55:05.281973Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "860be6de4a369d650343e5f8b9553111",
     "grade": false,
     "grade_id": "cell-5a461f0a132ec91c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "#from sklearn ...\n",
    "\n",
    "#predictor= ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:55:06.792955Z",
     "start_time": "2021-11-08T09:55:06.766402Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d213dc9a9008d1cf54d07a84c8d25b2",
     "grade": true,
     "grade_id": "cell-87d690a661bb2ea0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Tests\n",
    "\n",
    "# Testing the precision in the training set\n",
    "yh=predictor.predict(X_train)\n",
    "train_score=metrics.accuracy_score(y_true=y_train, y_pred=yh)\n",
    "if len(predictor.steps)<2:\n",
    "    print(\"The predictor is not a pipeline. Did you forget scaling?\")\n",
    "assert(len(predictor.steps)>=2)\n",
    "print(train_score)\n",
    "assert(train_score > 0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Evaluation of the predictor\n",
    "\n",
    "Having trained the predictor, evaluate now it's performance using cross validation and test set. You may use `cross_val_score` function from the `sklearn.model_selection` library and `accuracy_score` from the `sklearn.metrics` library.\n",
    "\n",
    "Print also the confusion matrix to see which areas are miss-classified. Use `confusion_matrix` function from the `sklearn.metrics` library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:55:09.322079Z",
     "start_time": "2021-11-08T09:55:09.110300Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a9d6be77f62bfa109437e6621fe7490",
     "grade": false,
     "grade_id": "cell-279389e544f5241e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn ...\n",
    "#\n",
    "#cv_score= ...\n",
    "#train_score= ...\n",
    "#test_score= ...\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T09:55:11.100554Z",
     "start_time": "2021-11-08T09:55:11.098266Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0567804f0bfd01c6ce16519a484737b2",
     "grade": true,
     "grade_id": "cell-97372112517922a4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Tests\n",
    "assert(cv_score > 0.85)\n",
    "assert(train_score > 0.85)\n",
    "assert(test_score > 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Confusion matrix\n",
    "\n",
    "Make a confusion matrix of the classifier for training set, store it as `CM` and print it in screen for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87e085976a86ed13b1d0a069cefc65d7",
     "grade": false,
     "grade_id": "cell-6f274f3314325403",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b750a1fce8447c204e6a10dbb8f41d6e",
     "grade": true,
     "grade_id": "cell-f59fd038c1459eb1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if 'CM' not in globals():\n",
    "    print(\"Please store your confusion matrix as variable CM\")\n",
    "assert(CM.shape==(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a257839f5e94771ff910b16138342a32",
     "grade": false,
     "grade_id": "cell-ff616d9601f71903",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 5: Interpretation of the results\n",
    "\n",
    "1. What is your opinion of the precision you achieved?\n",
    "1. Can you see signs of overfitting? Why/Why not?\n",
    "1. Which samples were misclassified? To which class they were assigned?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7ea4687cfb6e265336ed6eb2c7523af",
     "grade": true,
     "grade_id": "cell-2400ed952e601287",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
